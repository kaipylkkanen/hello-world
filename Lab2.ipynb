{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Lab2.ipynb",
      "provenance": [],
      "private_outputs": true,
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kaipylkkanen/hello-world/blob/master/Lab2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QUaKMJUdZPtw",
        "colab_type": "text"
      },
      "source": [
        "# QTM 120: Lab 2\n",
        "\n",
        "In this lab we will focus on computing partial derivatives, numerically and symbolically. Finally, we will review the chain rule and explain how it relates to 'backpropogation' , neural networks, and AI.\n",
        "\n",
        "## Numerically computing partial derivatives\n",
        "Recall the definition of *the partial derivative with respect to $x$ of a function $f(x,y)$ at $(a,b)$*\n",
        "$$f_x(a,b):= \\lim_{\\Delta x \\rightarrow 0}\\frac{f(a+\\Delta x, b) - f(a,b)}{\\Delta x}$$\n",
        "\n",
        "Let's define a Python function below to approximate this value.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "C9qpEGThi9Vy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def approximate_f_x(f, a, b):\n",
        "  delta_x = 0.05\n",
        "  return (f(a + delta_x,b) - f(a,b))/delta_x\n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vms9I707nzwa",
        "colab_type": "text"
      },
      "source": [
        "## Example: partial derivative of $f(x,y)= x^2 + y^2$ with respect to $x$ at $(2,1)$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBicQqTnoDnK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def f(x,y):\n",
        "  return x**2 + y**2\n",
        "approximate_f_x(f,2,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M5ChuN9AoYni",
        "colab_type": "text"
      },
      "source": [
        "How did we do? \n",
        "\n",
        "We know that $f_x=2x$, and evaluating at $(2,1)$ gives 4. So we were accurate to only one decimal place.\n",
        "\n",
        "## Exercise: Modify the definition of `approximate_f_x` so that the approximation is improved.\n",
        "- How many more decimal places of accuracy can you achieve?\n",
        "\n",
        "- Hint: sufficient to change delta_x! "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Gi4ardUpkbF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qbZMhjCrpk5e",
        "colab_type": "text"
      },
      "source": [
        "## Symbolically computing partial derivatives\n",
        "\n",
        "Instead of using the definition, one can program the ''rules'' of differentiation and then find partial derivatives of common functions using these rules. This is what we mean by \"symbollically computing\" partial derivatives.\n",
        "\n",
        "For this we use some functions from the SymPy package (http://www.sympy.org).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LcFdKQG9rgMW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sympy import symbols, diff"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OWPF71dAtBgd",
        "colab_type": "text"
      },
      "source": [
        "We also need to tell SymPy what variables we will be using."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mt2gGzUotMko",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "x,y = symbols('x,y')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tAhRt-h1riQt",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "## Example: Compute $f_x$ symbolically\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ib6MvZBLq1YJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diff(f(x,y), x, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTZpPiAntZ2y",
        "colab_type": "text"
      },
      "source": [
        "## Example: Compute $f_{xx}$ symbolically\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4OXvsxd5uRmD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diff(f(x,y), x, 2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aogOvJASuafS",
        "colab_type": "text"
      },
      "source": [
        "# Computing (partial) integrals symbolically\n",
        "\n",
        "SymPy can also compute indefinite integrals by applying the ''rules'' we know from calculus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7scoe_AAuyr5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sympy import integrate"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "if1AMVE8u3CV",
        "colab_type": "text"
      },
      "source": [
        "## Example: Compute $\\int x^2 + y^2 dx$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PESBvhGtvLZ4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "integrate(f(x,y),x)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hhw6ofnwvPjj",
        "colab_type": "text"
      },
      "source": [
        "We can compute definite integrals by passing in the limits.\n",
        "\n",
        "## Example: Compute $\\int_0^1 x^2 + y^2 dx$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BuXZLZcSv3NJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "integrate(f(x,y),(x,0,1))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZAeo1egHwXGd",
        "colab_type": "text"
      },
      "source": [
        "# Example: Compute $\\iint_Rx^2+y^2dA$ where $R=[0,1]\\times [2,3]$"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mQMBW6qCwuAu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "integrate(f(x,y), (x,0,1), (y,2,3))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VQcJjEXvxDjF",
        "colab_type": "text"
      },
      "source": [
        "# Chain Rule\n",
        "\n",
        "Of course, as the `diff` function in `SymPy` \"knows\" all the rules of differentiation, it knows the chain rule. Let's check in the example below. We will compute the partial derivative of a composition of functions\n",
        "$$f(\\cos x,\\sin y)$$\n",
        "\n",
        "where $f(x,y)=x^2+y^2$. That is, we will compute\n",
        "\n",
        "$$\\frac{\\partial}{\\partial x}(\\cos(x)^2 + (\\sin(y)^2)$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GUoR9s61xX1e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sympy import cos, sin\n",
        "diff(f(cos(x), sin(y)), x, 1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sooPaSnaxrx2",
        "colab_type": "text"
      },
      "source": [
        "## Exercise: Does this agree with what you would find by hand? \n",
        "It should, this is an example of the sort of partial derivative computation you need to be comfortable working out on your own on the exam."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rucIWJWHzpCz",
        "colab_type": "text"
      },
      "source": [
        "## How to have a computer implement the chain rule?\n",
        "\n",
        "You might say, a computer already can implement it, that is what we just saw SymPy do. However, SymPy only knows how to apply the rules of differentiation to common functions such as polynomial and trigonometric functions. The function we have been working with, $f(x,y)=x^2+y^2$ is a polynomial function. The composition we just looked at was a composition of trigonometric and polynomial functions.\n",
        "\n",
        "What if you want to find the partial derivatives of any multivariable function? For instance, something like\n",
        "$$g(x,y):=\\text{ReLU}(x)+\\text{ReLU}(y)$$\n",
        "\n",
        "It is not a linear or even a polynomial function. Let's try SymPy now and see what happens.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GygU0V6j43qA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def ReLU(x):\n",
        "  return max(0,x)  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "taKOv3pK5PRB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def g(x,y):\n",
        "  return ReLU(x) + ReLU(y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xVVIoi2s6JOE",
        "colab_type": "text"
      },
      "source": [
        "When you try to execute the code below you will recieve an error because SymPy hasn't been told what rules to apply when differentiating the function `ReLU` with respect to x."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EXtzJLH65Wrj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "diff(g(x,y), x,1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iLqE9vpFz5oA",
        "colab_type": "text"
      },
      "source": [
        "## A third method for finding derivatives: *automatic differentiation*\n",
        "\n",
        "So we would like to program a means of finding the partial derivatives, and more generally, the gradient vector, of more general functions which are compositions of functions, and to do so as quickly and accurately as possible.\n",
        "\n",
        "In general, there are many ways to do this for many different types of functions. Roughly speaking, the name given to various solutions to this problem is *automatic differentiation*. \n",
        "\n",
        "The following links are not required course material, but are presented so that students interested in machine learning can appreciate how the topics we are learning are used in those subjects.\n",
        "\n",
        "Here is a link to the introduction of a book on the topic that introduces automatic differentiation.\n",
        "\n",
        "https://doi.org/10.1137/1.9780898717761.ch1\n",
        "\n",
        "and here is another introduction in the context of data science and machine learning \n",
        "\n",
        "https://arxiv.org/abs/1502.05767\n",
        "\n",
        "What does this have to do with data science?\n",
        "\n",
        "**Backpropogation** is the name given to an algorithm for computing the gradient vector of a family of multivariable functions called **Neural networks** which are fundamental to AI and data science. Backpropagation is a special case of what is sometimes called * reverse accumulation mode automatic differentiation* (see the arxiv paper above).\n",
        "\n",
        "If you are interested in machine learning, here is an intuitive description of backpropogation:\n",
        "\n",
        "http://cs231n.github.io/optimization-2/\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cYOut9XooCBp",
        "colab_type": "text"
      },
      "source": [
        "# Lab 2 Exercises\n",
        "You are to work together with a **new** group over the next week to solve 8 different textbook exercises on the computer. The group will be assigned like last time by me during lab. Put your solutions together into one .ipynb file, choose \"save a copy as a GitHub Gist...\" and then submit the link to me for your lab submission. Be sure to nicely format it, with a title and names of all group members at the top, and headings (using markdown #, or ##) in text cells to present the solutions like they are presented here."
      ]
    }
  ]
}